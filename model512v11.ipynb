{"cells":[{"cell_type":"markdown","source":["# HE prediction\n","This tutorial shows how to construct a testing workflow of HE prediction task.\n","- Data: ATACH-2 + Yale\n","- Train data: Trained 507 patients (HE=83) and validated 127 patients (HE=21).\n","- Test data: 159 patients\n","- AUC = 0.79\n","\n","And it contains below features:\n","- CT input with thick thickness\n","- Pre-processing: extract brain window, skull stripping, remove boundary, resample\n","- Segmentation and Prediction: 3D SegResNet model, DenseNet.\n"],"metadata":{"id":"u0ktdz7XPACO"}},{"cell_type":"markdown","source":["## Setup environment"],"metadata":{"id":"I34NXDrSSf7-"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir(\"drive/My Drive/bkHEprediction\")\n","!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zlYGjLQXShK8","executionInfo":{"status":"ok","timestamp":1686758443530,"user_tz":240,"elapsed":9541,"user":{"displayName":"Tuan Tran","userId":"03196305730134920723"}},"outputId":"ade0be0d-f919-4c58-a5d5-0b5ad06ff248"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","2023-06-14 16:00:38.845953: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-06-14 16:00:39.893465: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}]},{"cell_type":"markdown","source":["## Setup data directory\n","\n","You can specify a directory with variable.\n","This allows you to save results and reuse for the next step.\n","\n","## Setup function for segmentation\n","**getbrainwindow(nii)** extract the brain window from CT with window_center=40, window_width=80\n","- Input: nii which load the CT\n","- Output: numpy array image [0,1]\n","\n","**removeSkull(img1)** skull stripping\n","- Input: numpy array image\n","- Output: brain region (numpy array image [0,1])\n","\n","**getDilation(img1, img1seg)** return dilation of objects inside image\n","\n","**resizeImgResizeWithPadOrCrop(img1, size):** return image with pad or crop to the size\n","\n","## Setup path variable\n"],"metadata":{"id":"SjjTcnAnSqfD"}},{"cell_type":"code","source":["import monai\n","from monai.data import  DataLoader, ImageDataset\n","from monai.transforms import (\n","    Resize, NormalizeIntensity, Activations, Compose, EnsureType, CenterSpatialCrop,ScaleIntensity,ResizeWithPadOrCrop, ResizeWithPadOrCropd,\n","    LoadImaged, EnsureChannelFirstd, EnsureTyped, NormalizeIntensityd, ScaleIntensityd,AddChannel\n",")\n","from monai.data import CacheDataset, DataLoader, ImageDataset, Dataset\n","from monai.data import decollate_batch\n","from monai.networks.nets import DenseNet121\n","from skimage.morphology import disk, binary_dilation, binary_erosion, remove_small_objects\n","import pandas as pd\n","import numpy as np\n","import nibabel as nib\n","import torch\n","import scipy.ndimage as nd\n","import os\n","import cv2\n","import matplotlib.pyplot as plt\n","import matplotlib\n","import nibabel.processing\n","from monai.utils import set_determinism\n","import random\n","import scipy\n","random.seed(123)\n","set_determinism(seed=123)\n","from monai.metrics import DiceMetric\n","from monai.networks.nets import SegResNet\n","from MyDenseNet import MyDenseNet121\n","from monai.inferers import sliding_window_inference\n","from sklearn.metrics import (\n","    classification_report, confusion_matrix,\n","    ConfusionMatrixDisplay\n",")\n","VAL_AMP = True\n","def inference(input):\n","    def _compute(input):\n","        return sliding_window_inference(\n","            inputs=input,\n","            roi_size=(256, 256, 32),\n","            sw_batch_size=4,\n","            predictor=model,\n","            overlap=0.5,\n","        )\n","\n","    if VAL_AMP:\n","        with torch.cuda.amp.autocast():\n","            return _compute(input)\n","    else:\n","        return _compute(input)\n","from monai.transforms import (\n","    Activations, AsDiscrete,\n","    Compose\n",")\n","import warnings\n","warnings.filterwarnings('ignore')\n","import time\n","import os.path\n","from os import path\n","X=0\n","Y=1\n","Z=2\n","def resample3d(image, spacing, new_spacing):\n","    resize_x = spacing[X] / new_spacing[X]\n","    new_shape_x = np.round(image.shape[X] * resize_x)\n","    resize_x = float(new_shape_x) / float(image.shape[X])\n","    sx = spacing[X] / resize_x\n","\n","    resize_y = spacing[Y] / new_spacing[Y]\n","    new_shape_y = np.round(image.shape[Y] * resize_y)\n","    resize_y = new_shape_y / image.shape[Y]\n","    sy = spacing[Y] / resize_y\n","\n","    resize_z = spacing[Z] / new_spacing[Z]\n","    new_shape_z = np.round(image.shape[Z] * resize_z)\n","    resize_z = float(new_shape_z) / float(image.shape[Z])\n","    sz = spacing[Z] / resize_z\n","\n","    image = scipy.ndimage.interpolation.zoom(image, (resize_x, resize_y, resize_z), order=1)\n","\n","    return image\n","\n","\n","def getbrainwindow(nii):\n","    window_center, window_width = 40, 80\n","    tmpimg1 = nii.get_fdata()\n","    tmpimg1[tmpimg1 < 0] = 0\n","    tmpimg1[tmpimg1 > 200] = 0\n","    img_min = window_center - window_width // 2\n","    img_max = window_center + window_width // 2\n","    tmpimg1[tmpimg1 < img_min] = img_min\n","    tmpimg1[tmpimg1 > img_max] = img_max\n","    tmpimg1 = (tmpimg1 - tmpimg1.min()) / np.ptp(tmpimg1)\n","    return tmpimg1\n","\n","def removeSkull(img1):\n","    img_bw = img1.copy()\n","    img_bw[img_bw > 0] = 1\n","    for slice in range(0, img_bw.shape[2]):\n","        if slice < round(img_bw.shape[2] / 7) or slice > (img_bw.shape[2] - round(img_bw.shape[2] / 7)):\n","            img_bw[:, :, slice] = 0\n","        if img_bw[:, :, slice].sum() > 0:\n","            img_bw[:, :, slice] = binary_erosion(img_bw[:, :, slice].astype(np.uint8),\n","                                                 disk(4, dtype=bool))\n","            img_bw[:, :, slice] = remove_small_objects(img_bw[:, :, slice].astype(bool), 5000)\n","            img_bw[:, :, slice] = binary_dilation(img_bw[:, :, slice].astype(np.uint8),\n","                                                  disk(4, dtype=bool))\n","            img_bw[:, :, slice] = nd.binary_fill_holes(img_bw[:, :, slice].astype(np.uint8))\n","        if img_bw[:, :, slice].sum() > 0:\n","            contours, _ = cv2.findContours(img_bw[:, :, slice].astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","            mask = np.zeros(img_bw[:, :, slice].shape, dtype=np.uint8)\n","            cv2.drawContours(mask, contours, -1, (255, 255, 255), -1)\n","            img_bw[:, :, slice][mask == 0] = 0\n","            mask = np.zeros(img_bw[:, :, slice].shape, dtype=np.uint8)\n","            cv2.drawContours(mask, contours, -1, (255, 255, 255), 20)\n","            img_bw[:, :, slice][mask == 255] = 0\n","    img1[img_bw == 0] = 0\n","    img1 = 1.0 * (img1 - img1.min()) / np.ptp(img1)\n","    return img1\n","\n","def getDilation(img1, img1seg):\n","    if img1seg.sum() <= 0:\n","        return img1seg\n","    img1dilation = img1.copy()\n","    img_bw = img1seg.copy()\n","    for slice in range(0, img_bw.shape[2]):\n","        if img_bw[:, :, slice].sum() > 0:\n","            img_bw[:, :, slice] = binary_dilation(img_bw[:, :, slice].astype(np.uint8),\n","                                                  disk(10, dtype=bool))\n","    img1dilation[img_bw == 0] = 0\n","    img1dilation = 1.0 * (img1dilation - img1dilation.min()) / np.ptp(img1dilation)\n","    return img1dilation\n","\n","\n","def resizeImgResizeWithPadOrCrop(img1, size):\n","    resizeImage = ResizeWithPadOrCrop(spatial_size=size)\n","    image = np.zeros([1, img1.shape[0], img1.shape[1], img1.shape[2]])\n","    image[0, :, :, :] = img1\n","    image = resizeImage(image)\n","    return image\n","\n","def selectGroupSlice(tmpimg1, tmpimg1seg):\n","    # find the first slice which contains hematoma\n","    for slice1 in range(tmpimg1seg.shape[2]):\n","        if tmpimg1seg[:, :, slice1].sum() > 0:\n","            break\n","    # find the last slice which contains hematoma\n","    for slice2 in range(tmpimg1seg.shape[2] - 1, 0, -1):\n","        if tmpimg1seg[:, :, slice2].sum() > 0:\n","            break\n","    if slice1 < slice2:\n","        slice1 = slice1 - 16\n","        if slice1 < 0:\n","            slice1 = 0\n","        slice2 = slice2 + 16\n","        if slice2 > tmpimg1seg.shape[2] - 1:\n","            slice2 = tmpimg1seg.shape[2] - 1\n","        if slice2 - slice1 >= 96:\n","            slice2 = slice1 + 96\n","        img1 = np.zeros([tmpimg1.shape[0], tmpimg1.shape[1], 96])\n","        img1seg = np.zeros([tmpimg1seg.shape[0], tmpimg1seg.shape[1], 96])\n","        img1[:, :, 0:slice2 - slice1] = tmpimg1[:, :, slice1:slice2]\n","        img1seg[:, :, 0:slice2 - slice1] = tmpimg1seg[:, :, slice1:slice2]\n","    else:\n","        img1 = np.zeros([tmpimg1.shape[0], tmpimg1.shape[1], 96])\n","        img1[:, :, 0:64] = tmpimg1[:, :,\n","                           int(np.round(tmpimg1.shape[2] / 2) - 32):int(np.round(tmpimg1.shape[2] / 2) + 32)]\n","        img1seg = np.zeros([tmpimg1seg.shape[0], tmpimg1seg.shape[1], 96])\n","    img1 = 1.0 * (img1 - img1.min()) / np.ptp(img1)\n","    return img1, img1seg\n","\n","data_dir = \"italy_ds/\"\n","baseline_path = \"baseline\"\n","output_path = data_dir + \"preprocessing\"\n","output_path_pre_classification = data_dir + \"classification\"\n","raw_data = pd.read_csv(os.path.join(data_dir, \"labels.csv\"), sep=\";\")\n","weight_segmentation_path = \"full_6000_new_best_metric_model_segmentation3d_1_1.pth\"\n","weight_classification_path = \"6000x5auc_classification3d_110.pth\""],"metadata":{"id":"_y0CIuJpPEVJ","executionInfo":{"status":"ok","timestamp":1686758473338,"user_tz":240,"elapsed":6463,"user":{"displayName":"Tuan Tran","userId":"03196305730134920723"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["##Preprocessing for segmentation\n","- Input: CT files in italy_ds/baseline, mask in italy_ds/mask\n","- Output: brain window _brain.nii.gz and groundtruth _ich_seg.nii.gz with size (512x512x48) in folder italy_ds/preprocessing\n","Code extract brain window, skull stripping and remove small objects, boundary. - ResizeWithPadOrCrop to (512x512x48)"],"metadata":{"id":"Jge782VgT4pj"}},{"cell_type":"code","source":["total_start = time.time()\n","for _, c_row in raw_data.iterrows():\n","    patient = str(c_row['filename'])\n","    if path.exists(os.path.join(output_path, patient + \"_brain.nii.gz\")) == True:\n","        continue\n","    print(\"Processing: \", patient)\n","    pathCT1 = os.path.join(data_dir, baseline_path, patient + \".nii.gz\")\n","    nii = nib.load(pathCT1)\n","    img1 = getbrainwindow(nii)\n","    spacing = nii.header['pixdim'][1:4]\n","    if img1.shape[2] > 48:\n","        new_spacing = [spacing[0], spacing[1], 5]\n","        img1 = resample3d(img1, spacing, new_spacing)\n","    else:\n","        new_spacing = spacing\n","    img1 = removeSkull(img1)\n","    image = resizeImgResizeWithPadOrCrop(img1, (512, 512, 48))\n","    empty_header = nib.Nifti1Header()\n","    affine = np.eye(4)\n","    affine[:3, :3] = np.diag(new_spacing)\n","    clipped_img = nib.Nifti1Image(image[0, :, :, :], affine, empty_header)\n","    nib.save(clipped_img, os.path.join(output_path, patient + \"_brain.nii.gz\"))\n","total_time = time.time() - total_start\n","print(\"Finish: \", total_time)"],"metadata":{"id":"a7h-dJWUS6Ah","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b0de49b8-8706-4396-9af9-4bdb0be360ac","executionInfo":{"status":"ok","timestamp":1686758300177,"user_tz":240,"elapsed":119,"user":{"displayName":"Tuan Tran","userId":"03196305730134920723"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Finish:  0.0040149688720703125\n"]}]},{"cell_type":"markdown","source":["##Segmentation\n","- Input: brain window _brain.nii.gz and groundtruth _ich_seg.nii.gz with size (512x512x48) in folder italy_ds/preprocessing\n","- Output: hematoma region in folder italy_ds/preprocessing/*_seg_auto.nii.gz with size (512x512x48)\n","- Using SegResNet model"],"metadata":{"id":"oLKUp_eaUdVi"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"oVJ9_n8OZSlq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"01b30ef0-d72b-4d71-ff92-2375ca79759c","executionInfo":{"status":"ok","timestamp":1686758690934,"user_tz":240,"elapsed":11797,"user":{"displayName":"Tuan Tran","userId":"03196305730134920723"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Finish:  11.653442144393921\n"]}],"source":["total_start = time.time()\n","xtest = []\n","nametest = []\n","for _, c_row in raw_data.iterrows():\n","    xtest = np.append(xtest, os.path.join(output_path, str(c_row['filename']) + \"_brain.nii.gz\"))\n","    nametest = np.append(nametest, str(c_row['filename']))\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","test_transforms = Compose(\n","    [\n","        LoadImaged(keys=[\"image\"]),\n","        EnsureChannelFirstd(keys=[\"image\"]),\n","        EnsureTyped(keys=[\"image\"]),\n","        NormalizeIntensityd(keys=\"image\"),\n","        ScaleIntensityd(keys=\"image\"),\n","    ]\n",")\n","num_batch = 1\n","test_files = [{\"image\": t2Img, \"name\": name} for t2Img, name in\n","              zip(xtest, nametest)]\n","test_ds = Dataset(data=test_files, transform=test_transforms)\n","test_loader = DataLoader(test_ds, batch_size=num_batch, shuffle=False,\n","                         num_workers=num_batch, pin_memory=torch.cuda.is_available())\n","model = SegResNet(\n","    blocks_down=[1, 2, 2, 4],\n","    blocks_up=[1, 1, 1],\n","    init_filters=16,\n","    in_channels=1,\n","    out_channels=1,\n","    dropout_prob=0.2,\n",").to(device)\n","model.load_state_dict(\n","    torch.load(os.path.join(weight_segmentation_path)))\n","\n","post_trans = Compose(\n","    [Activations(sigmoid=True), AsDiscrete(threshold=0.5)]\n",")\n","\n","model.eval()\n","with torch.no_grad():\n","    num = 0\n","    for val_data in test_loader:\n","        if path.exists(os.path.join(output_path, str(val_data[\"name\"][0]) + \"_seg_auto.nii.gz\"))==True:\n","              continue\n","        val_inputs = val_data[\"image\"].to(device)\n","        val_outputs = inference(val_inputs)\n","        val_outputs = post_trans(val_outputs)\n","        for p in range(num_batch):\n","            print(num, \"Patient:\", val_data[\"name\"][p])\n","            num = num + 1\n","            t1 = val_outputs[p, 0, :, :, :].cpu().numpy()\n","            nii = nib.load(os.path.join(output_path, str(val_data[\"name\"][p]) + \"_brain.nii.gz\"))\n","            empty_header = nib.Nifti1Header()\n","            affine = np.eye(4)\n","            affine[:3, :3] = np.diag((nii.header[\"pixdim\"])[1:4])\n","            pathCT1seg = os.path.join(output_path, str(val_data[\"name\"][p]) + \"_seg_auto.nii.gz\")\n","            imgseg = nib.Nifti1Image(t1, affine, empty_header)\n","            nib.save(imgseg, pathCT1seg)\n","total_time = time.time() - total_start\n","print(\"Finish: \", total_time)"]},{"cell_type":"markdown","source":["##Preprocessing for classification\n","- Input: CT files in italy_ds/baseline, hematoma region in italy_ds/preprocessing/*_seg_auto.nii.gz\n","- Output: brain windows and hematoma region in folder italy_ds/classification/*_brain_seg_dilation.nii.gz with size (2x128x128x96)\n","- Code: Get brain window from preprocessing of segmentation step, resample. Select group of slices containing Hematoma\n","- Resize to 2x192x192x96\n","- CenterSpatialCrop to 2x128x128x96"],"metadata":{"id":"9WrcCb2LUsC5"}},{"cell_type":"code","source":["total_start = time.time()\n","xtest = []\n","ytest = []\n","nametest = []\n","for _, c_row in raw_data.iterrows():\n","    patient = str(c_row['filename'])\n","    if path.exists(os.path.join(output_path_pre_classification, patient + \"_brain_seg_dilation.nii.gz\")) == True:\n","        continue\n","    print(\"Patient:\" + patient)\n","    pathCT1seg = os.path.join(output_path, str(c_row['filename']) + \"_seg_auto.nii.gz\")\n","    nii = nib.load(pathCT1seg)\n","    spacing = (nii.header[\"pixdim\"])[1:4]\n","    voxel_size = [1, 1, 1]\n","    tmpimg1seg = nii.get_fdata()\n","    for i in range(tmpimg1seg.shape[2]):\n","        if np.sum(tmpimg1seg[:, :, i]) < 50:\n","            tmpimg1seg[:, :, i] = 0\n","    tmpimg1seg = resample3d(tmpimg1seg, spacing, voxel_size)\n","    tmpimg1seg[tmpimg1seg > 0] = 1\n","\n","    pathCT1 = os.path.join(output_path, patient + \"_brain.nii.gz\")\n","    nii = nib.load(pathCT1)\n","    tmpimg1 = nii.get_fdata()\n","    tmpimg1 = resample3d(tmpimg1, spacing, voxel_size)\n","\n","    img1, img1seg = selectGroupSlice(tmpimg1, tmpimg1seg)\n","    img1dilation = getDilation(img1, img1seg)\n","\n","    image = np.zeros([2, img1.shape[0], img1.shape[1], img1.shape[2]])\n","    image[0, :, :, :] = img1\n","    image[1, :, :, :] = img1dilation\n","    resizeImage = Resize([192, 192, 96])\n","    cropImage = CenterSpatialCrop(roi_size=(128, 128, 96))\n","    scaleImage = ScaleIntensity()\n","    image = resizeImage(image)\n","    image = cropImage(image)\n","    image = scaleImage(image)\n","    # save file\n","    pathCT1clipseg = os.path.join(data_dir, \"classification\", patient + \"_brain_seg_dilation.nii.gz\")\n","    empty_header = nib.Nifti1Header()\n","    nii = nib.load(pathCT1)\n","    clipped_img = nib.Nifti1Image(image, nii.affine, empty_header)\n","    nib.save(clipped_img, pathCT1clipseg)\n","total_time = time.time() - total_start\n","print(\"Finish: \", total_time)"],"metadata":{"id":"RAy9P3MnykwL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3fd46f3b-75b3-4e0c-dec5-322c36499f5d","executionInfo":{"status":"ok","timestamp":1686758516671,"user_tz":240,"elapsed":160,"user":{"displayName":"Tuan Tran","userId":"03196305730134920723"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Finish:  0.0036656856536865234\n"]}]},{"cell_type":"markdown","source":["##Classification\n","- Input: brain windows and hematoma region in folder italy_ds/classification/*_brain_seg_dilation.nii.gz (2x128x128x96)\n","- Output: AUC\n","- Code: using MyDenseNet121 which is developed from DenseNet121\n"],"metadata":{"id":"DUwGGftPU1if"}},{"cell_type":"code","source":["total_start = time.time()\n","pathFilenames = []\n","labels = []\n","names = []\n","for _, c_row in raw_data.iterrows():\n","    pathFilenames.append(os.path.join(output_path_pre_classification, str(c_row['filename']) + \"_brain_seg_dilation.nii.gz\"))\n","    labels.append(c_row['label'])\n","    names.append(str(c_row['filename']))\n","labels = np.asarray(labels).astype(int)\n","pathFilenames = np.asarray(pathFilenames)\n","names = np.asarray(names)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = MyDenseNet121(spatial_dims=3, in_channels=2, out_channels=128).to(device)\n","model.load_state_dict(torch.load(weight_classification_path))\n","model.eval()\n","xval = pathFilenames\n","yval = labels\n","val_ds = ImageDataset(\n","    image_files=xval, labels=yval)\n","val_loader = DataLoader(val_ds, batch_size=10, shuffle=False,\n","                        num_workers=10, pin_memory=torch.cuda.is_available())\n","y_pred_trans = Compose([EnsureType(), Activations(sigmoid=True)])\n","y_trans = Compose([EnsureType()])\n","with torch.no_grad():\n","    y_pred = torch.tensor([], dtype=torch.float32, device=device)\n","    y = torch.tensor([], dtype=torch.double, device=device)\n","    for val_data in val_loader:\n","        inputs, val_labels = val_data[0].to(\n","            device), val_data[1].to(device)\n","        val_outputs = model(inputs)\n","        y_pred1 = val_outputs.flatten()\n","        y1 = val_labels\n","        y_pred = torch.cat([y_pred, y_pred1], dim=0)\n","        y = torch.cat([y, y1], dim=0)\n","    y_onehot = [y_trans(i) for i in decollate_batch(y)]\n","    y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n","    auc_metric1 = monai.metrics.ROCAUCMetric()\n","    auc_metric1(y_pred_act, y_onehot)\n","    print(\"AUC total= \", auc_metric1.aggregate())\n","    auc_metric1.reset()\n","total_time = time.time() - total_start\n","print(\"Finish: \", total_time)\n"],"metadata":{"id":"4Up055RkynrM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686758653953,"user_tz":240,"elapsed":3302,"user":{"displayName":"Tuan Tran","userId":"03196305730134920723"}},"outputId":"e2f3f9c0-0c18-452c-ed66-c457231dd590"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["AUC total=  0.8333333333333334\n","Finish:  3.0871572494506836\n"]}]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}